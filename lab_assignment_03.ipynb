{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niharika-Ravela/Niharika_INFO5502_Summer2022/blob/main/lab_assignment_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAxw6TsEE74V"
      },
      "source": [
        "## The third Lab-assignment (07/22/2022 11:59'AM' - 07/26/2022 11:59PM, 50 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX3K9Z-hE74c"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wHtlbldE74e"
      },
      "source": [
        "Question 1 (10 points). Fomulate your domain problem: Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZS7f3IIAE74f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "8e9ede12-ce06-4bb3-c0b6-e4c9a71df342"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nPlease write you answer here:\\nI am considering the IMDb website to get the Top TV Shows from the website. The result is to get the top rated 250 TV Shows.\\nTo answer this question we have to scrape the data from the 'https://www.imdb.com/chart/toptv/?ref_=nv_tvv_250' \\n\\nFor answering this kind of problem we need to scrape the raw data from the direct website. For my problem I can use 250 data points for the analysis.\\nThe steps that I took are:\\n1. First, to scrape the data we need to have a library called BeautifulSoup and requests.\\n2. After that we need to give the URL and store it in a variable so that we can parse the data.\\n3. After saving the url of the website we need to request the website for the data using the requests library.\\n4. Next, Use the BeautifulSoup to get the HTML i.e., the raw data from the website.\\n5. To get the exact tag that we want we have to use Soup.findall method\\n6. Finally, we have to remove all HTML tags and parse individually and convert into plain text.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "\n",
        "Please write you answer here:\n",
        "I am considering the IMDb website to get the Top TV Shows from the website. The result is to get the top rated 250 TV Shows.\n",
        "To answer this question we have to scrape the data from the 'https://www.imdb.com/chart/toptv/?ref_=nv_tvv_250' \n",
        "\n",
        "For answering this kind of problem we need to scrape the raw data from the direct website. For my problem I can use 250 data points for the analysis.\n",
        "The steps that I took are:\n",
        "1. First, to scrape the data we need to have a library called BeautifulSoup and requests.\n",
        "2. After that we need to give the URL and store it in a variable so that we can parse the data.\n",
        "3. After saving the url of the website we need to request the website for the data using the requests library.\n",
        "4. Next, Use the BeautifulSoup to get the HTML i.e., the raw data from the website.\n",
        "5. To get the exact tag that we want we have to use Soup.findall method\n",
        "6. Finally, we have to remove all HTML tags and parse individually and convert into plain text.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MPcX385E74i"
      },
      "source": [
        "Question 2 (30 points). Collect your data to answer the research problem: Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VlZCHNoME74k"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.imdb.com/chart/top/\n",
        "from bs4 import BeautifulSoup\n",
        "import requests, openpyxl\n",
        "excel = openpyxl.Workbook()\n",
        "sheet = excel.active\n",
        "sheet.title = 'IMDb Top TV Shows'\n",
        "sheet.append(['Rank','TV Show Name','Year of release','IMDB Rating'])\n",
        "try:\n",
        "    html = requests.get('https://www.imdb.com/chart/toptv/?ref_=nv_tvv_250')\n",
        "    html.raise_for_status()\n",
        "\n",
        "    soup = BeautifulSoup(html.text,'html.parser')\n",
        "    tvshows = soup.find('tbody',class_='lister-list').find_all('tr')\n",
        "    for i in tvshows:\n",
        "        name = i.find('td',class_='titleColumn').a.text\n",
        "        rank = i.find('td',class_='titleColumn').get_text(strip=True).split('.')[0]\n",
        "        year = i.find('td',class_='titleColumn').span.text.strip('()')\n",
        "        rating = i.find('td',class_='ratingColumn imdbRating').strong.text\n",
        "        print(rank,name, year, rating)\n",
        "        sheet.append([rank,name, year, rating])\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAGI1gPuMaFh",
        "outputId": "39287ed5-789f-4a19-f882-0060aecc3247"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Planet Earth II 2016 9.4\n",
            "2 Breaking Bad 2008 9.4\n",
            "3 Planet Earth 2006 9.4\n",
            "4 Band of Brothers 2001 9.4\n",
            "5 Chernobyl 2019 9.3\n",
            "6 The Wire 2002 9.3\n",
            "7 Blue Planet II 2017 9.2\n",
            "8 Avatar: The Last Airbender 2005 9.2\n",
            "9 Cosmos: A Spacetime Odyssey 2014 9.2\n",
            "10 The Sopranos 1999 9.2\n",
            "11 Cosmos 1980 9.2\n",
            "12 Our Planet 2019 9.2\n",
            "13 Game of Thrones 2011 9.2\n",
            "14 Rick and Morty 2013 9.1\n",
            "15 The World at War 1973 9.1\n",
            "16 Fullmetal Alchemist: Brotherhood 2009 9.1\n",
            "17 The Last Dance 2020 9.0\n",
            "18 Life 2009 9.0\n",
            "19 Sherlock 2010 9.0\n",
            "20 The Twilight Zone 1959 9.0\n",
            "21 The Vietnam War 2017 9.0\n",
            "22 Batman: The Animated Series 1992 9.0\n",
            "23 Scam 1992: The Harshad Mehta Story 2020 9.0\n",
            "24 Arcane 2021 8.9\n",
            "25 The Blue Planet 2001 8.9\n",
            "26 Firefly 2002 8.9\n",
            "27 The Office 2005 8.9\n",
            "28 Attack on Titan 2013 8.9\n",
            "29 Human Planet 2011 8.9\n",
            "30 Frozen Planet 2011 8.9\n",
            "31 Death Note 2006 8.9\n",
            "32 True Detective 2014 8.9\n",
            "33 Only Fools and Horses 1981 8.9\n",
            "34 The Beatles: Get Back 2021 8.9\n",
            "35 Hunter x Hunter 2011 8.9\n",
            "36 The Civil War 1990 8.9\n",
            "37 Seinfeld 1989 8.9\n",
            "38 Persona 2018 8.9\n",
            "39 Fargo 2014 8.9\n",
            "40 Clarkson's Farm 2021 8.9\n",
            "41 Dekalog 1989 8.9\n",
            "42 Cowboy Bebop 1998 8.9\n",
            "43 Friends 1994 8.8\n",
            "44 When They See Us 2019 8.8\n",
            "45 Gravity Falls 2012 8.8\n",
            "46 Nathan for You 2013 8.8\n",
            "47 Last Week Tonight with John Oliver 2014 8.8\n",
            "48 Africa 2013 8.8\n",
            "49 Monty Python's Flying Circus 1969 8.8\n",
            "50 Apocalypse: The Second World War 2009 8.8\n",
            "51 TVF Pitchers 2015 8.8\n",
            "52 Better Call Saul 2015 8.8\n",
            "53 It's Always Sunny in Philadelphia 2005 8.8\n",
            "54 Taskmaster 2015 8.8\n",
            "55 Das Boot 1985 8.8\n",
            "56 Curb Your Enthusiasm 2000 8.8\n",
            "57 Fawlty Towers 1975 8.8\n",
            "58 Pride and Prejudice 1995 8.8\n",
            "59 Freaks and Geeks 1999 8.8\n",
            "60 The West Wing 1999 8.8\n",
            "61 Leyla and Mecnun 2011 8.7\n",
            "62 Blackadder Goes Forth 1989 8.7\n",
            "63 Twin Peaks 1990 8.7\n",
            "64 Black Mirror 2011 8.7\n",
            "65 Narcos 2015 8.7\n",
            "66 Chappelle's Show 2003 8.7\n",
            "67 BoJack Horseman 2014 8.7\n",
            "68 Dragon Ball Z 1989 8.7\n",
            "69 Succession 2018 8.7\n",
            "70 Dragon Ball Z 1989 8.7\n",
            "71 I, Claudius 1976 8.7\n",
            "72 Peaky Blinders 2013 8.7\n",
            "73 South Park 1997 8.7\n",
            "74 Rome 2005 8.7\n",
            "75 Ted Lasso 2020 8.7\n",
            "76 Oz 1997 8.7\n",
            "77 Six Feet Under 2001 8.7\n",
            "78 One Piece 1999 8.7\n",
            "79 Over the Garden Wall 2014 8.7\n",
            "80 Dark 2017 8.7\n",
            "81 The Mandalorian 2019 8.7\n",
            "82 Kota Factory 2019 8.7\n",
            "83 Arrested Development 2003 8.7\n",
            "84 Steins;Gate 2011 8.7\n",
            "85 One Punch Man 2015 8.7\n",
            "86 As If 2021 8.7\n",
            "87 The Simpsons 1989 8.7\n",
            "88 The Boys 2019 8.7\n",
            "89 House M.D. 2004 8.7\n",
            "90 Stranger Things 2016 8.7\n",
            "91 The Shield 2002 8.7\n",
            "92 Battlestar Galactica 2004 8.6\n",
            "93 Fleabag 2016 8.6\n",
            "94 Downton Abbey 2010 8.6\n",
            "95 Invincible 2021 8.6\n",
            "96 Mad Men 2007 8.6\n",
            "97 House of Cards 2013 8.6\n",
            "98 Vinland Saga 2019 8.6\n",
            "99 The Marvelous Mrs. Maisel 2017 8.6\n",
            "100 The Crown 2016 8.6\n",
            "101 Star Trek: The Next Generation 1987 8.6\n",
            "102 Sarabhai V/S Sarabhai 2004 8.6\n",
            "103 Peep Show 2003 8.6\n",
            "104 The Adventures of Sherlock Holmes 1984 8.6\n",
            "105 Friday Night Lights 2006 8.6\n",
            "106 Top Gear 2002 8.6\n",
            "107 Severance 2022 8.6\n",
            "108 Gullak 2019 8.6\n",
            "109 1883 2021 8.6\n",
            "110 Panchayat 2020 8.6\n",
            "111 The Grand Tour 2016 8.6\n",
            "112 Berserk 1997 8.6\n",
            "113 The Thick of It 2005 8.6\n",
            "114 Line of Duty 2012 8.6\n",
            "115 Justice League Unlimited 2004 8.6\n",
            "116 Deadwood 2004 8.6\n",
            "117 Dexter 2006 8.6\n",
            "118 The X-Files 1993 8.6\n",
            "119 Parks and Recreation 2009 8.6\n",
            "120 The Jinx: The Life and Deaths of Robert Durst 2015 8.6\n",
            "121 Father Ted 1995 8.6\n",
            "122 Blackadder II 1986 8.6\n",
            "123 Naruto: Shippûden 2007 8.6\n",
            "124 Heartstopper 2022 8.6\n",
            "125 This Is Us 2016 8.6\n",
            "126 The Bridge 2011 8.6\n",
            "127 Demon Slayer: Kimetsu no Yaiba 2019 8.6\n",
            "128 Aspirants 2021 8.6\n",
            "129 Daredevil 2015 8.6\n",
            "130 Code Geass 2006 8.6\n",
            "131 Archer 2009 8.6\n",
            "132 Atlanta 2016 8.6\n",
            "133 Behzat Ç.: Bir Ankara Polisiyesi 2010 8.6\n",
            "134 It's a Sin 2021 8.6\n",
            "135 Mystery Science Theater 3000 1988 8.6\n",
            "136 Haikyuu!! 2014 8.6\n",
            "137 Blackadder the Third 1987 8.6\n",
            "138 Young Justice 2010 8.6\n",
            "139 Lonesome Dove 1989 8.6\n",
            "140 The Return of Sherlock Holmes 1986 8.6\n",
            "141 Mahabharat 1988 8.6\n",
            "142 Gomorrah 2014 8.5\n",
            "143 Mindhunter 2017 8.5\n",
            "144 Yellowstone 2018 8.5\n",
            "145 QI 2003 8.5\n",
            "146 Adventure Time 2010 8.5\n",
            "147 Making a Murderer 2015 8.5\n",
            "148 Monster 2004 8.5\n",
            "149 Mr. Bean 1990 8.5\n",
            "150 Dopesick 2021 8.5\n",
            "151 Poirot 1989 8.5\n",
            "152 Anne with an E 2017 8.5\n",
            "153 Justified 2010 8.5\n",
            "154 Boardwalk Empire 2010 8.5\n",
            "155 Yeh Meri Family 2018 8.5\n",
            "156 The Haunting of Hill House 2018 8.5\n",
            "157 The Bugs Bunny Show 1960 8.5\n",
            "158 Formula 1: Drive to Survive 2019 8.5\n",
            "159 Yes Minister 1980 8.5\n",
            "160 The Queen's Gambit 2020 8.5\n",
            "161 The Newsroom 2012 8.5\n",
            "162 The Eric Andre Show 2012 8.5\n",
            "163 Primal 2019 8.5\n",
            "164 Westworld 2016 8.5\n",
            "165 Flight of the Conchords 2007 8.5\n",
            "166 Ramayan 1987 8.5\n",
            "167 Dragon Ball 1986 8.5\n",
            "168 Impractical Jokers 2011 8.5\n",
            "169 Justice League 2001 8.5\n",
            "170 Spaced 1999 8.5\n",
            "171 Coupling 2000 8.5\n",
            "172 Battlestar Galactica 2003 8.5\n",
            "173 Dragon Ball 1995 8.5\n",
            "174 Crash Landing on You 2019 8.5\n",
            "175 Homicide: Life on the Street 1993 8.5\n",
            "176 The Venture Bros. 2003 8.5\n",
            "177 Long Way Round 2004 8.5\n",
            "178 The IT Crowd 2006 8.5\n",
            "179 The Office 2001 8.5\n",
            "180 The Bureau 2015 8.5\n",
            "181 What We Do in the Shadows 2019 8.5\n",
            "182 Samurai Champloo 2004 8.5\n",
            "183 Louie 2010 8.5\n",
            "184 The Family Man 2019 8.5\n",
            "185 Mr. Robot 2015 8.5\n",
            "186 I'm Alan Partridge 1997 8.5\n",
            "187 Black Sun 2017 8.5\n",
            "188 Yes, Prime Minister 1986 8.5\n",
            "189 Whose Line Is It Anyway? 1998 8.5\n",
            "190 Through the Wormhole 2010 8.5\n",
            "191 Shameless 2011 8.5\n",
            "192 Pose 2018 8.5\n",
            "193 Cobra Kai 2018 8.5\n",
            "194 Letterkenny 2016 8.5\n",
            "195 Spartacus 2010 8.5\n",
            "196 Jujutsu Kaisen 2020 8.5\n",
            "197 The Offer 2022 8.5\n",
            "198 Samurai Jack 2001 8.5\n",
            "199 Fullmetal Alchemist 2003 8.5\n",
            "200 Brass Eye 1997 8.5\n",
            "201 Sons of Anarchy 2008 8.5\n",
            "202 Skam 2015 8.5\n",
            "203 North & South 2004 8.5\n",
            "204 Spartacus: Gods of the Arena 2011 8.5\n",
            "205 Neon Genesis Evangelion 1995 8.5\n",
            "206 Silicon Valley 2014 8.5\n",
            "207 Trailer Park Boys 2001 8.5\n",
            "208 Ezel 2009 8.5\n",
            "209 From the Earth to the Moon 1998 8.5\n",
            "210 Doctor Who 2005 8.5\n",
            "211 Futurama 1999 8.5\n",
            "212 Endeavour 2012 8.5\n",
            "213 Twin Peaks 2017 8.5\n",
            "214 Rurouni Kenshin: Trust and Betrayal 1999 8.5\n",
            "215 Detectorists 2014 8.5\n",
            "216 Mob Psycho 100 2016 8.5\n",
            "217 Spy x Family 2022 8.5\n",
            "218 Hannibal 2013 8.5\n",
            "219 Community 2009 8.4\n",
            "220 The Expanse 2015 8.4\n",
            "221 Schitt's Creek 2015 8.4\n",
            "222 Generation Kill 2008 8.4\n",
            "223 Chef's Table 2015 8.4\n",
            "224 The Night Of 2016 8.4\n",
            "225 The Angry Video Game Nerd 2004 8.4\n",
            "226 Alfred Hitchcock Presents 1955 8.4\n",
            "227 Big Little Lies 2017 8.4\n",
            "228 Regular Show 2009 8.4\n",
            "229 The Knick 2014 8.4\n",
            "230 Content Cop 2015 8.4\n",
            "231 Mr Inbetween 2018 8.4\n",
            "232 Your Lie in April 2014 8.4\n",
            "233 Wentworth 2013 8.4\n",
            "234 John Adams 2008 8.4\n",
            "235 Black Books 2000 8.4\n",
            "236 My Brilliant Friend 2018 8.4\n",
            "237 The Punisher 2017 8.4\n",
            "238 X-Men: The Animated Series 1992 8.4\n",
            "239 The Defiant Ones 2017 8.4\n",
            "240 Avrupa Yakasi 2004 8.4\n",
            "241 Mare of Easttown 2021 8.4\n",
            "242 Southland 2009 8.4\n",
            "243 Garth Marenghi's Darkplace 2004 8.4\n",
            "244 Erased 2016 8.4\n",
            "245 I Love Lucy 1951 8.4\n",
            "246 Clannad: After Story 2008 8.4\n",
            "247 Ozark 2017 8.4\n",
            "248 Gintama 2005 8.4\n",
            "249 Harley Quinn 2019 8.4\n",
            "250 Foyle's War 2002 8.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdm1ZhKNE74l"
      },
      "source": [
        "Question 3 (10 points). Understand the data quality: Search a second hand dataset (any dataset) from kaggle or other websites. Describe the data quality problem of the dataset and explain your strtegy to clean the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z73oH8eVE74n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "0cacf2e8-ea71-4a59-ba1f-3d9a5e2b0368"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nPlease write you answer here:\\n\\nFrom Kaggle I have considered a dataset called:Customer Personality Analysis. \\nCustomer personality analysis is a detailed analysis of a company's ideal customers. \\nThis helps companies better understand their customers and makes it easier  to tailor their products  to the specific needs, behaviors, and concerns of different types of customers. \\nCustomer personality analysis helps companies modify their products based on  target customers from different types of customer segments. \\nFor example, instead of spending money to sell a new product to all customers in the company's database, the company analyzes which customer segment is most likely to buy the product and  only to that particular segment. You can sell the product.\\nFrom the dataset there are some features that needs a little work. \\nApart from that there are also missing values that can be replaced by the either imputyation or interpolation but in this case we can use imputation.\\nFor that first we need to find the analysis of the feature depending on that we can use either mean or mode or median values to replace the missing values or some cases we can simply replace them by NaN. \\nAfter replacing all the missing values then we need to check if all the features are usefull or not.\\nIn this case I can tellm that for 'Maritial_Status' There are marrie, divoreced, single and two ohter types-YOLO and Absurd. In my opinion we can just consider them as the single type.\\nNot only this there are also few variable that count the Amt individually for each category we can just combine all of them into one variable mentioning the Amount spent.\\nBy doing these steps we can make sure that the data is clean and we can proceed to the next steps by performing the data analysis and forming research questions.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "'''\n",
        "\n",
        "Please write you answer here:\n",
        "\n",
        "From Kaggle I have considered a dataset called:Customer Personality Analysis. \n",
        "Customer personality analysis is a detailed analysis of a company's ideal customers. \n",
        "This helps companies better understand their customers and makes it easier  to tailor their products  to the specific needs, behaviors, and concerns of different types of customers. \n",
        "Customer personality analysis helps companies modify their products based on  target customers from different types of customer segments. \n",
        "For example, instead of spending money to sell a new product to all customers in the company's database, the company analyzes which customer segment is most likely to buy the product and  only to that particular segment. You can sell the product.\n",
        "From the dataset there are some features that needs a little work. \n",
        "Apart from that there are also missing values that can be replaced by the either imputyation or interpolation but in this case we can use imputation.\n",
        "For that first we need to find the analysis of the feature depending on that we can use either mean or mode or median values to replace the missing values or some cases we can simply replace them by NaN. \n",
        "After replacing all the missing values then we need to check if all the features are usefull or not.\n",
        "In this case I can tellm that for 'Maritial_Status' There are marrie, divoreced, single and two ohter types-YOLO and Absurd. In my opinion we can just consider them as the single type.\n",
        "Not only this there are also few variable that count the Amt individually for each category we can just combine all of them into one variable mentioning the Amount spent.\n",
        "By doing these steps we can make sure that the data is clean and we can proceed to the next steps by performing the data analysis and forming research questions.\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c6a7f839effdddf35473dacbb9a8184e57ebbba5133cab03b12e4f28d4f0d0f"
      }
    },
    "colab": {
      "name": "lab_assignment_03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}