{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niharika-Ravela/Niharika_INFO5502_Summer2022/blob/main/lab_assignment_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAxw6TsEE74V"
      },
      "source": [
        "## The third Lab-assignment (07/22/2022 11:59'AM' - 07/26/2022 11:59PM, 50 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX3K9Z-hE74c"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wHtlbldE74e"
      },
      "source": [
        "Question 1 (10 points). Fomulate your domain problem: Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "ZS7f3IIAE74f",
        "outputId": "8e9ede12-ce06-4bb3-c0b6-e4c9a71df342"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nPlease write you answer here:\\nI am considering the IMDb website to get the Top 1000 movies in the year 2021 from the website. The result is to get the top rated films.\\nTo answer this question we have to scrape the data from the 'https://www.imdb.com/search/title/?release_date=2021-01-01,2021-12-31&sort=boxoffice_gross_us,desc&start=' \\n\\nFor answering this kind of problem we need to scrape the raw data from the direct website. For my problem I can use 1000 data points for the analysis.\\nThe steps that I took are:\\n1. First, to scrape the data we need to have a library called BeautifulSoup and requests.\\n2. After that we need to give the URL and store it in a variable so that we can parse the data.\\n3. After saving the url of the website we need to request the website for the data using the requests library.\\n4. Next, Use the BeautifulSoup to get the HTML i.e., the raw data from the website.\\n5. To get the exact tag that we want we have to use Soup.findall method\\n6. Finally, we have to remove all HTML tags and parse individually and convert into plain text.\\n\\nIn particular in this case since I was using the IMDb Website not only this one but every site can have multiple pages to extract data from multiple pages we need to first\\nuse a base by getting one data point in this case also I first tried to get the first top movie by its all details.\\nNext, I tried creating functions that iterate through the pages i.e., it can happend by finding the element from the inspect part of the website we can get the exact element we are looking for.\\nOnce I got the data points I stored them in the dataframe. But while iterating I used some counters to keep track of the page numbers and the number of titles that are being collected.\\n\\n\\n\""
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "I am considering the IMDb website to get the Top 1000 movies in the year 2021 from the website. The result is to get the top rated films.\n",
        "To answer this question we have to scrape the data from the 'https://www.imdb.com/search/title/?release_date=2021-01-01,2021-12-31&sort=boxoffice_gross_us,desc&start=' \n",
        "\n",
        "For answering this kind of problem we need to scrape the raw data from the direct website. For my problem I can use 1000 data points for the analysis.\n",
        "The steps that I took are:\n",
        "1. First, to scrape the data we need to have a library called BeautifulSoup and requests.\n",
        "2. After that we need to give the URL and store it in a variable so that we can parse the data.\n",
        "3. After saving the url of the website we need to request the website for the data using the requests library.\n",
        "4. Next, Use the BeautifulSoup to get the HTML i.e., the raw data from the website.\n",
        "5. To get the exact tag that we want we have to use Soup.findall method\n",
        "6. Finally, we have to remove all HTML tags and parse individually and convert into plain text.\n",
        "\n",
        "In particular in this case since I was using the IMDb Website not only this one but every site can have multiple pages to extract data from multiple pages we need to first\n",
        "use a base by getting one data point in this case also I first tried to get the first top movie by its all details.\n",
        "Next, I tried creating functions that iterate through the pages i.e., it can happend by finding the element from the inspect part of the website we can get the exact element we are looking for.\n",
        "Once I got the data points I stored them in the dataframe. But while iterating I used some counters to keep track of the page numbers and the number of titles that are being collected.\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MPcX385E74i"
      },
      "source": [
        "Question 2 (30 points). Collect your data to answer the research problem: Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: translate in c:\\users\\billa\\anaconda3\\lib\\site-packages (3.6.1)\n",
            "Requirement already satisfied: libretranslatepy==2.1.1 in c:\\users\\billa\\anaconda3\\lib\\site-packages (from translate) (2.1.1)\n",
            "Requirement already satisfied: lxml in c:\\users\\billa\\anaconda3\\lib\\site-packages (from translate) (4.6.3)\n",
            "Requirement already satisfied: click in c:\\users\\billa\\anaconda3\\lib\\site-packages (from translate) (8.0.3)\n",
            "Requirement already satisfied: requests in c:\\users\\billa\\anaconda3\\lib\\site-packages (from translate) (2.26.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\billa\\anaconda3\\lib\\site-packages (from click->translate) (0.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\billa\\anaconda3\\lib\\site-packages (from requests->translate) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\billa\\anaconda3\\lib\\site-packages (from requests->translate) (1.26.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\billa\\anaconda3\\lib\\site-packages (from requests->translate) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\billa\\anaconda3\\lib\\site-packages (from requests->translate) (2.0.4)\n",
            "Requirement already satisfied: googletrans==3.1.0a0 in c:\\users\\billa\\anaconda3\\lib\\site-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in c:\\users\\billa\\anaconda3\\lib\\site-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: chardet==3.* in c:\\users\\billa\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: certifi in c:\\users\\billa\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.10.8)\n",
            "Requirement already satisfied: sniffio in c:\\users\\billa\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\billa\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in c:\\users\\billa\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: idna==2.* in c:\\users\\billa\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: hstspreload in c:\\users\\billa\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.7.10)\n",
            "Requirement already satisfied: h2==3.* in c:\\users\\billa\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\billa\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\billa\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\billa\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install translate\n",
        "!pip install googletrans==3.1.0a0\n",
        "\n",
        "import bs4\n",
        "import requests\n",
        "import time\n",
        "import random as ran\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Movie Name: Spider-Man: No Way Home \n",
            "Release Year: 2021 \n",
            "IMDb Rating: 8.3 \n",
            "Meta score: 71.0 \n",
            "Votes: 688,957\n",
            "List of top 1000 movies:s from: 901 - 950 | remaining count: 000\n",
            "\n",
            "                                          name\n",
            "0                      Spider-Man: No Way Home\n",
            "1    Shang-Chi and the Legend of the Ten Rings\n",
            "2                  Venom: Let There Be Carnage\n",
            "3                                  Black Widow\n",
            "4                            F9: The Fast Saga\n",
            "..                                         ...\n",
            "944                                Do, Re & Mi\n",
            "945                                Do, Re & Mi\n",
            "946                                Do, Re & Mi\n",
            "947                                Do, Re & Mi\n",
            "948                                Do, Re & Mi\n",
            "\n",
            "[949 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "url = 'https://www.imdb.com/search/title/?release_date=2021-01-01,2021-12-31&sort=boxoffice_gross_us,desc&start='\n",
        "\n",
        "source = requests.get(url).text\n",
        "soup = bs4.BeautifulSoup(source,'html.parser')\n",
        "\n",
        "movie_box = soup.findAll('div',{'class':'lister-item-content'})\n",
        "#print(movie_box)\n",
        "\n",
        "mov_name = movie_box[0].find('a').get_text() # Name of the movie\n",
        "result = translator.translate(mov_name, dest = 'en')\n",
        "mov_name = result.text\n",
        "m_reyear = int(movie_box[0].find('span',{'class': 'lister-item-year'}).contents[0][1:-1]) # Release year\n",
        "\n",
        "m_rating = float(movie_box[0].find('div',{'class':'inline-block ratings-imdb-rating'}).get('data-value')) #rating\n",
        "\n",
        "m_mscore = float(movie_box[0].find('span',{'class':'metascore favorable'}).contents[0].strip()) #meta score\n",
        "\n",
        "m_votes = int(movie_box[0].find('span',{'name':'nv'}).get('data-value')) # votes\n",
        "\n",
        "print(\"Movie Name: \" + mov_name,\n",
        "      \"\\nRelease Year: \" + str(m_reyear),\n",
        "      \"\\nIMDb Rating: \" + str(m_rating),\n",
        "      \"\\nMeta score: \" + str(m_mscore),\n",
        "      \"\\nVotes: \" + '{:,}'.format(m_votes)\n",
        "\n",
        ")  # For just one instance\n",
        "\n",
        "\n",
        "#For all the multiple pages coding\n",
        "\n",
        "def scrape_mblock(movie_block):\n",
        "    \n",
        "    movieb_data ={}\n",
        "  \n",
        "    try:\n",
        "        movieb_data['name'] = movie_block.find('a').get_text() # Name of the movie\n",
        "        result = translator.translate(movieb_data['name'], dest = 'en')\n",
        "        movieb_data['name'] = result.text\n",
        "    except:\n",
        "        movieb_data['name'] = None\n",
        "\n",
        "\n",
        "    return movieb_data\n",
        "\n",
        "\n",
        "\n",
        "def scrape_m_page(movie_box):\n",
        "    \n",
        "    page_movie_data = []\n",
        "    num_blocks = len(movie_box)\n",
        "    \n",
        "    for block in range(num_blocks):\n",
        "        page_movie_data.append(scrape_mblock(movie_box[block]))\n",
        "    \n",
        "    return page_movie_data\n",
        "\n",
        "\n",
        "def scrape_this(link,t_count):\n",
        "\n",
        "    base_url = link\n",
        "    target = t_count\n",
        "    \n",
        "    start = 0\n",
        "    stop = 0\n",
        "    left = target - stop \n",
        "    \n",
        "    page_no = 1\n",
        "    \n",
        "    movie_data = []\n",
        "    \n",
        "    \n",
        "    while left > 0:\n",
        "\n",
        "        url = base_url + str(page_no)\n",
        "        source = requests.get(url).text\n",
        "        soup = bs4.BeautifulSoup(source,'html.parser')\n",
        "        \n",
        "        movie_box = soup.findAll('div',{'class':'lister-item-content'})\n",
        "        \n",
        "        movie_data.extend(scrape_m_page(movie_box))   \n",
        "        \n",
        "        start = int(soup.find(\"div\", {\"class\":\"nav\"}).find(\"div\", {\"class\": \"desc\"}).contents[1].get_text().split(\"-\")[0])\n",
        "\n",
        "        stop = int(soup.find(\"div\", {\"class\":\"nav\"}).find(\"div\", {\"class\": \"desc\"}).contents[1].get_text().split(\"-\")[1].split(\" \")[0])\n",
        "\n",
        "        left = target - stop\n",
        "        \n",
        "        print('\\r' + \"currently scraping movies from: \" + str(start) + \" - \"+str(stop), \"| remaining count: \" + str(left), flush=True, end =\"\")\n",
        "        \n",
        "        page_no = stop + 1\n",
        "           \n",
        "        time.sleep(ran.randint(0, 10))\n",
        "    \n",
        "    return movie_data\n",
        "\n",
        "\n",
        "base_scraping_link = \"https://www.imdb.com/search/title/?release_date=2021-01-01,2021-12-31&sort=boxoffice_gross_us,desc&start=\"\n",
        "\n",
        "top_movies = 1000 \n",
        "films = []\n",
        "\n",
        "movies = scrape_this(base_scraping_link,950)\n",
        "\n",
        "print('\\r'+\"List of top \" + str(top_movies) +\" movies:\" + \"\\n\", end=\"\\n\")\n",
        "movies=pd.DataFrame(movies)\n",
        "print(movies)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdm1ZhKNE74l"
      },
      "source": [
        "Question 3 (10 points). Understand the data quality: Search a second hand dataset (any dataset) from kaggle or other websites. Describe the data quality problem of the dataset and explain your strtegy to clean the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "z73oH8eVE74n",
        "outputId": "0cacf2e8-ea71-4a59-ba1f-3d9a5e2b0368"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n\\nPlease write you answer here:\\n\\nFrom Kaggle I have considered a dataset called:Customer Personality Analysis. \\nCustomer personality analysis is a detailed analysis of a company's ideal customers. \\nThis helps companies better understand their customers and makes it easier  to tailor their products  to the specific needs, behaviors, and concerns of different types of customers. \\nCustomer personality analysis helps companies modify their products based on  target customers from different types of customer segments. \\nFor example, instead of spending money to sell a new product to all customers in the company's database, the company analyzes which customer segment is most likely to buy the product and  only to that particular segment. You can sell the product.\\nFrom the dataset there are some features that needs a little work. \\nApart from that there are also missing values that can be replaced by the either imputyation or interpolation but in this case we can use imputation.\\nFor that first we need to find the analysis of the feature depending on that we can use either mean or mode or median values to replace the missing values or some cases we can simply replace them by NaN. \\nAfter replacing all the missing values then we need to check if all the features are usefull or not.\\nIn this case I can tellm that for 'Maritial_Status' There are marrie, divoreced, single and two ohter types-YOLO and Absurd. In my opinion we can just consider them as the single type.\\nNot only this there are also few variable that count the Amt individually for each category we can just combine all of them into one variable mentioning the Amount spent.\\nBy doing these steps we can make sure that the data is clean and we can proceed to the next steps by performing the data analysis and forming research questions.\\n\\n\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "'''\n",
        "\n",
        "Please write you answer here:\n",
        "\n",
        "From Kaggle I have considered a dataset called:Customer Personality Analysis. \n",
        "Customer personality analysis is a detailed analysis of a company's ideal customers. \n",
        "This helps companies better understand their customers and makes it easier  to tailor their products  to the specific needs, behaviors, and concerns of different types of customers. \n",
        "Customer personality analysis helps companies modify their products based on  target customers from different types of customer segments. \n",
        "For example, instead of spending money to sell a new product to all customers in the company's database, the company analyzes which customer segment is most likely to buy the product and  only to that particular segment. You can sell the product.\n",
        "From the dataset there are some features that needs a little work. \n",
        "Apart from that there are also missing values that can be replaced by the either imputyation or interpolation but in this case we can use imputation.\n",
        "For that first we need to find the analysis of the feature depending on that we can use either mean or mode or median values to replace the missing values or some cases we can simply replace them by NaN. \n",
        "After replacing all the missing values then we need to check if all the features are usefull or not.\n",
        "In this case I can tellm that for 'Maritial_Status' There are marrie, divoreced, single and two ohter types-YOLO and Absurd. In my opinion we can just consider them as the single type.\n",
        "Not only this there are also few variable that count the Amt individually for each category we can just combine all of them into one variable mentioning the Amount spent.\n",
        "By doing these steps we can make sure that the data is clean and we can proceed to the next steps by performing the data analysis and forming research questions.\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "lab_assignment_03.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "8b8ff65559d87b2fdc0067ced9865d279e8e4962e0bb98eb4c236dec2dee6d9e"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c6a7f839effdddf35473dacbb9a8184e57ebbba5133cab03b12e4f28d4f0d0f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
